import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.SparkConf;
//import org.apache.spark.log4j.Logger;
//import org.apache.spark.hadoop.io.LongeWritable;
//import org.apache.spark.hadoop.io.Text;
//import org.apache.spark.rdd.HadoopRDD;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.DataFrameReader;
import org.apache.spark.sql.DataFrameWriter;
import java.io.IOException;
//import com.databricks.spark.xml;

public class XMLTextMining {

    public static void main(String[] args) throws IOException {

         SparkConf conf = new SparkConf().setAppName("dbscan").set("spark.kryoserializer.buffer.mb","128").set("spark.akka.askTimeout", "6000").set("spark.akka.timeout", "6000").set("spark.worker.timeout", "6000").set("spark.akka.frameSize","64").set("spark.driver.memory","4g").set("spark.executor.memory","2g").set("spark.dynamicAllocation.enabled","false");
        final JavaSparkContext sc = new JavaSparkContext(conf);

         SQLContext sqlContext = new SQLContext(sc);
         //Dataset<Row> dataset = new Dataset<Row>();
         DataFrame dataset = null;

         String inputPath  = args[0];
         String outputPath = args[1];
         //long start_time =  System.currentTimeMillis(); 
         DataFrameReader dataframeh = sqlContext.read(); 
         dataframeh = dataframeh.format("com.databricks.spark.xml");
         //dataframeh = dataframeh.option("rowTag","book");
         dataframeh = dataframeh.option("rowTag","MedlineCitation");
         dataset = dataframeh.load(inputPath);
         System.out.println("--------------------------");
         //System.out.println("loading time: [" + (System.currentTimeMillis() - start_time) + "]");
         System.out.println("--------------------------");


         dataset.printSchema();
         // the num of rows
         int count = (int)dataset.count();

         //dataset.select("PMID","Article").show(count);
         
    }
}
         
