package mergesample;

import xml.XmlInputFormat;
import java.util.ArrayList;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.*;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.conf.Configuration;
import scala.Tuple2;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.*;
import java.io.IOException;
import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.io.FileOutputStream;
import java.io.BufferedOutputStream;
import java.io.File;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.StringWriter;
import java.net.URI;
import java.nio.file.Paths;

import java.nio.file.Files;
import java.nio.file.OpenOption;
import org.apache.hadoop.io.serializer.WritableSerialization;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;

import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.io.compress.GzipCodec;
import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.JavaRDD;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
import org.apache.hadoop.util.LineReader;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.Accumulator;

import java.util.Arrays;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Hashtable;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Queue;
import java.util.LinkedList;
import java.util.Vector;
import java.util.zip.GZIPInputStream;
import java.util.Set;
import java.util.Iterator;
import java.util.ListIterator;
import java.net.URI;
import scala.Tuple2;
public class MergeSample {

    public static void main(String[] args) throws IOException, InterruptedException {
        String inputPath  = args[0];
        String outputPath = args[1];
        String matrixFile = args[2];

        SparkConf sconf = new SparkConf().set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").set("spark.executor.memory", "2g");
        
        JavaSparkContext sc = new JavaSparkContext(sconf);
        Configuration conf = new Configuration();
        conf.set("xmlinput.start", "<MedlineCitation");
        conf.set("xmlinput.end", "</MedlineCitation>");
	JavaPairRDD<Text, Text> inputRDD = sc.newAPIHadoopFile(inputPath+"/*.gz", XmlInputFormat.class, Text.class, Text.class,conf);

        System.out.println(" NUM of inputRDD " + inputRDD.count());

	JavaRDD<ArrayList<Tuple2<String, String>>> stringRDD=	inputRDD.map(
            new Function<Tuple2<Text, Text>, ArrayList<Tuple2<String, String>>>() {
                public ArrayList<Tuple2<String, String>> call(Tuple2<Text, Text> t) {
			  
                    ArrayList<Tuple2<String, String>> list = new ArrayList<Tuple2<String, String>>();
System.out.println("KEY000 "+ t._1);
                    
                    String[] str = t._1.toString().split(":");
System.out.println("KEY "+ t._1.toString() + " str "+ str);
                    }

                    return list;
                    
                }
 
        });

//        System.out.println(" NUM of stringRDD " + stringRDD.count());

        JavaRDD<Tuple2<String, String>> flatSample = stringRDD.flatMap(
            new FlatMapFunction<ArrayList<Tuple2<String,String>>, Tuple2<String, String>>(){ 
                public Iterable<Tuple2<String, String>> call(ArrayList<Tuple2<String, String>> l){
                    return l;
                }
        });
   
        JavaPairRDD<String, String> sampleMapRDD = flatSample.mapToPair(
            new PairFunction<Tuple2<String, String>, String, String>() {
                public Tuple2<String, String> call(Tuple2<String, String> t){
                    return new Tuple2<String, String>(t._1, t._2);
                }
        });

        JavaPairRDD<String, String> cleanRDD = sampleMapRDD.filter ( 

             new Function<Tuple2<String, String>, Boolean>() {
                  public Boolean call(Tuple2<String, String> s) throws Exception {
                         if s.1.contains("##") || (s._1.contains("#CH")) {
                               return false;
                         } else {
                               return true;
                         }
                  }
        });
        
 
        System.out.println(" NUM of stringRDD " + sampleMapRDD.count());
		
        // read matrix file and create matrixRDD
        JavaRDD<String> input = sc.textFile(matrixFile);
        if (input == null){
            System.out.print(" input is null");
        } else {
            System.out.println("matrxi file reading is OK, length of input: " + input.count());
        }
        
        JavaRDD<String> inputRows = input.filter (
            new Function<String, Boolean>(){
                public Boolean call(String s) throws Exception {
                    if (s.contains("##")){
                        return false;
                    }
                    return true;
                }
        });

        int cnt = (int) inputRows.count();

System.out.println(" NUM of inputRows [" + inputRows.count() + "]");

        List<String> matrixHeader = inputRows.take(1);
        String[] mStr  =  matrixHeader.get(0).split("\t");
        ArrayList<String> realHeader = new ArrayList<String>(Arrays.asList(mStr));
        final Broadcast<ArrayList<String>> broadrealHeader = sc.broadcast(realHeader);

        JavaRDD<String> rowRDD = inputRows.filter (
            new Function<String, Boolean>(){
                 public Boolean call(String s) throws Exception {
                     if (s.contains("#CHROM")) {
                         return false;
                     } else {
                         return true;
                     }
                  }
        });
System.out.println(" NUM of rowRDD [" + rowRDD.count() + "]");

        
       JavaRDD<ArrayList<Tuple2<String, String>>> matrixRDD = rowRDD.map(
            new Function<String, ArrayList<Tuple2<String, String>>>() {
                public ArrayList<Tuple2<String, String>> call(String x) {
                    List<String> str = Arrays.asList(x.split("\t"));
                    
                    ArrayList<Tuple2<String, String>> iter = new ArrayList<Tuple2<String, String>>();
                    StringBuilder sb = new StringBuilder();

                    for (int k=0; k<9; k++){
                       sb.append(str.get(k) + "\t");
                    }

                    StringBuilder headStr = new StringBuilder("chr");
                    headStr = headStr.append(str.get(0));
                    headStr = headStr.append(":");
                    headStr = headStr.append(str.get(1));
                    headStr = headStr.append(":");
                    headStr = headStr.append(str.get(3));
                    headStr = headStr.append(str.get(4));
                    headStr = headStr.append(":");
                    for (int j = 9; j < str.size(); j++) { 
                        if (str.get(j).compareTo(".") == 0){
                           headStr = headStr.append(broadrealHeader.value().get(j)); 

                           sb = sb.append("!"+ j + "\t");

                           Tuple2<String, String> t2 = new Tuple2<String, String>(headStr.toString(), sb.toString());
                           iter.add(t2);
                        } else {
                           sb = sb.append(str.get(j) + "\t");
                        }
                        
                
                     }
                     return iter;
                   
                 }
       });

       System.out.println(" NUM of matrixRDD " + matrixRDD.count());

       JavaRDD<Tuple2<String, String>> flatRTN =  matrixRDD.flatMap(
           new FlatMapFunction<ArrayList<Tuple2<String, String>>, Tuple2<String, String>>(){
                public Iterable<Tuple2<String, String>> call(ArrayList<Tuple2<String, String>> l){
                    return l;
                }
        });

       System.out.println(" NUM of flatRTN " + flatRTN.count());

       JavaPairRDD<String, String> matrixMapRDD = flatRTN.mapToPair(
            new PairFunction<Tuple2<String, String>, String, String>() {
                public Tuple2<String, String> call(Tuple2<String, String> t) {
                    return new Tuple2<String,String>(t._1, t._2);
                }
        });

       System.out.println(" NUM of matrixMapRDD " + matrixMapRDD.count());

       JavaPairRDD<String, Tuple2<String, String>> jointResult = matrixMapRDD.join(sampleMapRDD);
       //jointResult.saveAsTextFile(outputPath);

       System.out.println("Joints function result: " + jointResult.collect());

       JavaPairRDD<String, Tuple2<String, String>> extractRDD = jointResult.mapToPair (
           new PairFunction<Tuple2<String, Tuple2<String, String>>, String, Tuple2<String, String>>() {
               public Tuple2<String, Tuple2<String, String>> call(Tuple2<String, Tuple2<String, String>> t) {
                   String[] str = t._1.split(":");
                   StringBuilder sb = new StringBuilder();
                   sb = sb.append(str[0] + ":" + str[1] + ":" + str[2]);
                   Tuple2<String, String> newT2 =  new Tuple2<String, String>(t._2._1, t._2._2);
                   Tuple2<String, Tuple2<String,String>> rtnTuple = new Tuple2<String, Tuple2<String, String>>(sb.toString(), newT2);
                   return rtnTuple;
                }
        });


 
      JavaPairRDD<String, Iterable<Tuple2<String, String>>> groupRDD = extractRDD.groupByKey();

      JavaRDD<String>  resultRDD = groupRDD.map( 
          new Function<Tuple2<String, Iterable<Tuple2<String, String>>>, String>() {
              public String call(Tuple2<String, Iterable<Tuple2<String, String>>> x) {
                  StringBuilder sb = new StringBuilder();
                  Iterator iter = x._2.iterator();

                  while (iter.hasNext()) {
                     Tuple2<String, String> t =(Tuple2<String, String>) iter.next();
                     String[] str = t._1.split("\t");
                     for (int i = 0; i < str.length - 1; i++) {
                        sb = sb.append(str[i] + "\t");
                     }
                     sb = sb.append(t._2 + "\t");
                  }

                  return sb.toString();
             } 
      });

      resultRDD.saveAsTextFile(outputPath);
         
    }
}
         
