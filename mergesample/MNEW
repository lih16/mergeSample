package mergesample;

import xml.XmlInputFormat;
import java.util.ArrayList;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.*;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.conf.Configuration;
import scala.Tuple2;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.*;
import java.io.IOException;
import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.io.FileOutputStream;
import java.io.BufferedOutputStream;
import java.io.File;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.StringWriter;
import java.net.URI;
import java.nio.file.Paths;

import java.nio.file.Files;
import java.nio.file.OpenOption;
import org.apache.hadoop.io.serializer.WritableSerialization;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;

import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.io.compress.GzipCodec;
import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.JavaRDD;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
import org.apache.hadoop.util.LineReader;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.Accumulator;

import java.util.Arrays;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Hashtable;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Queue;
import java.util.LinkedList;
import java.util.Vector;
import java.util.zip.GZIPInputStream;
import java.util.Set;
import java.util.Iterator;
import java.util.ListIterator;
import java.net.URI;
import scala.Tuple2;
public class MergeSample {

    public static void main(String[] args) throws IOException, InterruptedException {
        String inputPath  = args[0];
        String outputPath = args[1];
        String matrixFile = args[2];

        SparkConf sconf = new SparkConf().set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").set("spark.executor.memory", "2g");
        
        JavaSparkContext sc = new JavaSparkContext(sconf);
        Configuration conf = new Configuration();
        conf.set("xmlinput.start", "<MedlineCitation");
        conf.set("xmlinput.end", "</MedlineCitation>");
	JavaPairRDD<Text, Text> inputRDD = sc.newAPIHadoopFile(inputPath+"/*.gz", XmlInputFormat.class, Text.class, Text.class,conf);

//        inputRDD.saveAsTextFile(outputPath);
        System.out.println(" NUMofinputRDD " + inputRDD.count());

        JavaPairRDD<String, String> stringRDD=  inputRDD.mapToPair(new PairFunction<Tuple2<Text, Text>, String, String>() {
           public Tuple2<String, String> call(Tuple2<Text, Text> t) {

              return new Tuple2<String,String>(t._1.toString(), t._2.toString());

        }});

//      stringRDD.saveAsTextFile(outputPath);
       
        System.out.println(" NUMofsringRDD " + stringRDD.count());
        System.out.println(" NUMdistinctRDD " + stringRDD.distinct().count());

      JavaPairRDD<String, String> distinctRDD = stringRDD.distinct();

      JavaPairRDD<String, Iterable<String>> groupRDD = distinctRDD.groupByKey();

        System.out.println(" NUMofgroup RDD " + groupRDD.count());

      JavaRDD<String>  resultRDD = groupRDD.map( 
          new Function<Tuple2<String, Iterable<String>>, String>() {
              public String call(Tuple2<String, Iterable<String>> x) {
                  //System.out.println(" key " + x._1 + " value " + x._2); 

                  return x._1; 
             } 
      });

      resultRDD.saveAsTextFile(outputPath);
      //groupRDD.saveAsTextFile(outputPath);
         
    }
}
         
